{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Explore the BBC News archive\n",
    "\n",
    "Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrZevCPJ92HG",
    "outputId": "4ba10d10-1433-42fc-dc8e-83b297705cfe"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by looking at the structure of the csv that contains the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bbc-text.csv\", 'r') as csvfile:\n",
    "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n",
    "\n",
    "Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: remove_stopwords\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "    \n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "    \n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "    \n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    ### END CODE HERE\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "remove_stopwords(\"I am about to go to the store and get any snack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "'go store get snack'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the raw data\n",
    "\n",
    "Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n",
    "\n",
    "A couple of things to note:\n",
    "- You should omit the first line as it contains the headers and not data points.\n",
    "- There is no need to save the data points as numpy arrays, regular lists is fine.\n",
    "- To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n",
    "- `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n",
    "- Use the `remove_stopwords` function in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        filename (string): path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        ### START CODE HERE\n",
    "        reader = csv.reader(None, delimiter=None)\n",
    "        ### END CODE HERE\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "sentences, labels = parse_data_from_file(\"./bbc-text.csv\")\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "There are 2225 sentences in the dataset.\n",
    "\n",
    "First sentence has 436 words (after removing stopwords).\n",
    "\n",
    "There are 2225 labels in the dataset.\n",
    "\n",
    "The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Tokenizer\n",
    "\n",
    "Now it is time to tokenize the sentences of the dataset. \n",
    "\n",
    "Complete the `fit_tokenizer` below. \n",
    "\n",
    "This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences):\n",
    "    \"\"\"\n",
    "    Instantiates the Tokenizer class\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): lower-cased sentences without stopwords\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    # Instantiate the Tokenizer class by passing in the oov_token argument\n",
    "    tokenizer = None\n",
    "    # Fit on the sentences\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = fit_tokenizer(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "Vocabulary contains 29714 words\n",
    "\n",
    "<OOV> token included in vocabulary\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(tokenizer, sentences):\n",
    "    \"\"\"\n",
    "    Generates an array of token sequences and pads them to the same length\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
    "        sentences (list of string): list of sentences to tokenize and pad\n",
    "    \n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    # Convert sentences to sequences\n",
    "    sequences = None\n",
    "    \n",
    "    # Pad the sequences using the post padding strategy\n",
    "    padded_sequences = None\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
    "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
    "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
    "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "First padded sequence looks like this: \n",
    "\n",
    "[  96  176 1157 ...    0    0    0]\n",
    "\n",
    "Numpy array of all sequences has shape: (2225, 2438)\n",
    "\n",
    "This means there are 2225 sequences in total and each one has a size of 2438\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_labels(labels):\n",
    "    \"\"\"\n",
    "    Tokenizes the labels\n",
    "    \n",
    "    Args:\n",
    "        labels (list of string): labels to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Instantiate the Tokenizer class\n",
    "    # No need to pass additional arguments since you will be tokenizing the labels\n",
    "    label_tokenizer = None\n",
    "    \n",
    "    # Fit the tokenizer to the labels\n",
    "    \n",
    "    \n",
    "    # Save the word index\n",
    "    label_word_index = None\n",
    "    \n",
    "    # Save the sequences\n",
    "    label_sequences = None\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return label_sequences, label_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sequences, label_word_index = tokenize_labels(labels)\n",
    "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
    "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
    "\n",
    "First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rITvNKqT-51"
   },
   "source": [
    "**Congratulations on finishing this week's assignment!**\n",
    "\n",
    "You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
